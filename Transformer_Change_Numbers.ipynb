{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer Change Numbers.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOs/rsyJ38za95k0pN2cyrO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DominikBurkert/PyTorch/blob/master/Transformer_Change_Numbers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxbKk8NLBVvS"
      },
      "source": [
        "from google.colab import output\r\n",
        "#I don't know why nbextensions don't work with python3.6 \r\n",
        "with output.temporary():\r\n",
        "  !pip install --upgrade git+https://github.com/Kreijstal/colab_inspector.git \r\n",
        "  !python2.7 -m pip install --upgrade git+https://github.com/blois/colab_inspector.git #yes, really.\r\n",
        "  !jupyter nbextension install --py inspector\r\n",
        "\r\n",
        "import inspector\r\n",
        "\r\n",
        "# open a scratch cell (Ctrl+Alt+N)\r\n",
        "# run there\r\n",
        "# inspector.watch_globals()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgGzejYzBmHu"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_J-vTq8zBcMn"
      },
      "source": [
        "Daten"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvqKISjzBc2e"
      },
      "source": [
        "model = torch.nn.Transformer(d_model=10, nhead=1, num_encoder_layers=2, \r\n",
        "                     num_decoder_layers=2,  dropout=0.1, \r\n",
        "                     activation='relu', custom_encoder=None, custom_decoder=None)\r\n",
        "\r\n",
        "#wie soll der fehler berechnet werden? (softmax, mean-squared-error(MSELoss), CrossEntropyLoss)\r\n",
        "criterion = nn.MSELoss()\r\n",
        "\r\n",
        "\"\"\"Inputdaten für das Netz\"\"\"\r\n",
        "#input = torch.rand((10, 32, 512))\r\n",
        "#target = torch.rand((20, 32, 512))\r\n",
        "\r\n",
        "\"\"\"Inputdaten für das Netz\"\"\"\r\n",
        "x = [\r\n",
        "     [1,0,0,0,1,0,0,0,1,1],\r\n",
        "     [1,0,1,0,1,0,1,0,1,0],\r\n",
        "     [0,1,0,1,0,1,0,1,0,1],\r\n",
        "     [0,0,0,0,0,1,1,1,1,1],\r\n",
        "     [1,1,1,1,1,0,0,0,0,0],\r\n",
        "     [1,1,1,0,0,0,1,1,1,0]\r\n",
        "]\r\n",
        "input = torch.Tensor([x for _ in range(10)])\r\n",
        "#print(input)\r\n",
        "\r\n",
        "\"\"\"    Target-Daten (diese Daten sollen am Ende rauskommen -> hier: alle Zahlen einmal drehen. aus 0->1, aus 1->0) \"\"\"\r\n",
        "x = [\r\n",
        "     [0,1,1,1,0,1,1,1,0,0],\r\n",
        "     [0,1,0,1,0,1,0,1,0,1],\r\n",
        "     [1,0,1,0,1,0,1,0,1,0],\r\n",
        "     [1,1,1,1,1,0,0,0,0,0],\r\n",
        "     [0,0,0,0,0,1,1,1,1,1],\r\n",
        "     [0,0,0,1,1,1,0,0,0,1]\r\n",
        "]\r\n",
        "#print(x)\r\n",
        "target = torch.Tensor([x for _ in range(10)]) #target sind die Daten die man am ende haben will\r\n",
        "#print(target)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jbTPWOTJDBCd",
        "outputId": "c96f53b6-07aa-4271-a6e2-eb97d52c3e8b"
      },
      "source": [
        "for i in range(100):  \r\n",
        "  out = model(input, target) #einmal das netz durchlaufen\r\n",
        "  loss = criterion(out, target) #vergleich von output (vorhersage des netzes) und target(echte Werte)-> Berechnung des Fehlers\r\n",
        "  print(loss)\r\n",
        "\r\n",
        "  model.zero_grad()\r\n",
        "  loss.backward()\r\n",
        "  optimizer = optim.SGD(model.parameters(), lr=0.10)\r\n",
        "  optimizer.step()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(1.0277, grad_fn=<MseLossBackward>)\n",
            "tensor(0.7776, grad_fn=<MseLossBackward>)\n",
            "tensor(0.6951, grad_fn=<MseLossBackward>)\n",
            "tensor(0.6252, grad_fn=<MseLossBackward>)\n",
            "tensor(0.5624, grad_fn=<MseLossBackward>)\n",
            "tensor(0.5268, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4903, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4606, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4302, grad_fn=<MseLossBackward>)\n",
            "tensor(0.4012, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3906, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3682, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3397, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3199, grad_fn=<MseLossBackward>)\n",
            "tensor(0.3040, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2923, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2729, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2625, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2485, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2387, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2220, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2129, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2017, grad_fn=<MseLossBackward>)\n",
            "tensor(0.1926, grad_fn=<MseLossBackward>)\n",
            "tensor(0.1799, grad_fn=<MseLossBackward>)\n",
            "tensor(0.1756, grad_fn=<MseLossBackward>)\n",
            "tensor(0.1665, grad_fn=<MseLossBackward>)\n",
            "tensor(0.1570, grad_fn=<MseLossBackward>)\n",
            "tensor(0.1486, grad_fn=<MseLossBackward>)\n",
            "tensor(0.1357, grad_fn=<MseLossBackward>)\n",
            "tensor(0.1365, grad_fn=<MseLossBackward>)\n",
            "tensor(0.1295, grad_fn=<MseLossBackward>)\n",
            "tensor(0.1183, grad_fn=<MseLossBackward>)\n",
            "tensor(0.1146, grad_fn=<MseLossBackward>)\n",
            "tensor(0.1105, grad_fn=<MseLossBackward>)\n",
            "tensor(0.1034, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0979, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0976, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0918, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0846, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0841, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0823, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0754, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0747, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0681, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0650, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0677, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0650, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0605, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0565, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0556, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0527, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0505, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0475, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0504, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0451, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0464, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0442, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0402, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0425, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0395, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0396, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0387, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0349, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0386, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0350, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0321, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0334, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0337, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0320, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0318, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0322, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0306, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0262, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0289, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0272, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0284, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0267, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0266, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0268, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0267, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0253, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0257, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0245, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0260, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0242, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0216, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0244, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0246, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0222, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0233, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0202, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0224, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0231, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0211, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0204, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0190, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0200, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0198, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0199, grad_fn=<MseLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "id": "euXxyb9iCPgJ",
        "outputId": "cbbaf6a9-b928-48a0-f1cd-447d76d1d633"
      },
      "source": [
        "print(\"Vorhersage: \\n\", torch.round(out[1,5]))\r\n",
        "print(\"Ziel: \\n\", target[1,5])"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vorhersage: \n",
            " tensor([0., 0., 0., 1., 1., 1., 0., 0., 0., 1.], grad_fn=<RoundBackward>)\n",
            "Ziel: \n",
            " tensor([0., 0., 0., 1., 1., 1., 0., 0., 0., 1.])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}